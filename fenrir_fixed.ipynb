{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyaanshvats/finrir/blob/main/fenrir_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS9FLb1lN8xI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiClWgANODk1"
      },
      "outputs": [],
      "source": [
        "TAGS = [\"bash\", \"git\", \"tar\", \"grep\", \"venv\"]\n",
        "BASE_URL = \"https://api.stackexchange.com/2.3\"\n",
        "SITE = \"stackoverflow\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyFuFZGBOFJy"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_questions(tag, pagesize=50):\n",
        "    url = f\"{BASE_URL}/questions\"\n",
        "    params = {\n",
        "        \"order\": \"desc\",\n",
        "        \"sort\": \"votes\",\n",
        "        \"tagged\": tag,\n",
        "        \"site\": SITE,\n",
        "        \"pagesize\": pagesize,\n",
        "        \"filter\": \"!9_bDDxJY5\",  # includes body\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    try:\n",
        "        return response.json().get(\"items\", [])\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\")\n",
        "        print(response.text)\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9gIUZgMOGk_"
      },
      "outputs": [],
      "source": [
        "def get_best_answer(question_id):\n",
        "    url = f\"{BASE_URL}/questions/{question_id}/answers\"\n",
        "    params = {\n",
        "        \"order\": \"desc\",\n",
        "        \"sort\": \"votes\",\n",
        "        \"site\": SITE,\n",
        "        \"filter\": \"!9_bDE(fI5\",  # includes body\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if not response.text:  # Check for empty response\n",
        "        print(f\"Empty response received for question ID {question_id}. Possible rate limit issue.\")\n",
        "        return None\n",
        "    answers = response.json().get(\"items\", [])\n",
        "    if not answers:\n",
        "        return None\n",
        "    return answers[0][\"body\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q1MWiJROIpX",
        "outputId": "2e96f9cb-4b64-408e-8886-caa9214f9fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting questions for tag: bash\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:07<00:00,  1.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting questions for tag: git\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:09<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting questions for tag: tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:08<00:00,  1.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting questions for tag: grep\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:08<00:00,  1.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting questions for tag: venv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:08<00:00,  1.80it/s]\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "data = []\n",
        "\n",
        "for tag in TAGS:\n",
        "    print(f\"Collecting questions for tag: {tag}\")\n",
        "    questions = get_questions(tag, pagesize=60)\n",
        "    for q in tqdm(questions):\n",
        "        if not q.get(\"is_answered\"):\n",
        "            continue\n",
        "        q_text = BeautifulSoup(q[\"title\"], \"html.parser\").get_text()\n",
        "        a_html = get_best_answer(q[\"question_id\"])\n",
        "        if not a_html:\n",
        "            continue\n",
        "        a_text = BeautifulSoup(a_html, \"html.parser\").get_text()\n",
        "        data.append({\n",
        "            \"question\": q_text.strip(),\n",
        "            \"answer\": a_text.strip()\n",
        "        })\n",
        "        time.sleep(1)  # increased sleep time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w6SpQnrOKhP",
        "outputId": "8b036e43-e1a0-4d50-857b-361b4825c213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Collected 248 Q&A pairs.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "with open(\"data/questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Collected {len(data)} Q&A pairs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liw1IF_yOMUE",
        "outputId": "2f598f71-124e-45ad-a950-caf0e35069d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers peft accelerate datasets evaluate bitsandbytes --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557,
          "referenced_widgets": [
            "538f8fdd540a487c9d026f93d096ff79",
            "340d7dc30d81416695ad43d8deecc2a6",
            "a0ab418b2ec1461686b2ba35eee3f63f",
            "da43006e14a54c9ea33ebc4620f524cb",
            "bf5536b2e5534124aabaa57c54c45857",
            "228a7cae0b0f4ca48acc0867683f440e",
            "4920cd7355a84d16b1c05e378eec07ac",
            "2b6c599195b3499f9a16ea60743c4c0a",
            "5cae3f76ab964c0aa7aaaa6f20f8f699",
            "930880bf986b4fc6bb2433f9d1f0d2c4",
            "af3c312294034c24b32a2c7001076afb",
            "18799d4351de4836aaedf9c2beb6107e",
            "f2d84b3f82524331bf74513736338597",
            "754c6c8d829c4e78a11a9005937b64a6",
            "45d1ed910e5a4538beeb30cab5375333",
            "2ca2ed06f7fe431d9a21bbf8df1dd0ea",
            "bcf855af68924109931a89ea65709f4f",
            "4eaafaf61e3145e09be9e46c3f8ee06d",
            "50f97c38a07942d6be94f2340fea9b92",
            "f4d11e1fda264db8a316413d47c00249",
            "4cc7c2465a404aa7a3ac1d117ea9a65b",
            "20a9f7bcbbbc44938c2c4310fa91c21a",
            "c849432085314300a2590e41a3f636d4",
            "d044ed196ae34294878ef0c3e0cdd288",
            "21216d0e3aeb47f4ae9888bc5fea2772",
            "84ba2023745a4af7b9d37830b2a13fbb",
            "29e998feb8c54fb2b626c110c979d0b7",
            "9a8e7dc0db9e4080ac063374271c829a",
            "dad03ba8490043c09605fb081fc622dc",
            "1b5e9bce42964b97920f3d29da034d6f",
            "e5622abff3424ac294d3e33463ebcd1d",
            "5e81d6e2833c4f4aae29e17d90ac2686",
            "592560de760349a588758e1ed413d672",
            "13c2db9e782d4c1bb3d109ed436c6a04",
            "ddfd7133b1714f6d8f3b6f7b4098de50",
            "8e7990a1158b4ef4bca2481f6494af97",
            "9bf083b6a5ba402a8ccf693edb35bddb",
            "6cd9bc450fa14101aa69517b4389ea30",
            "836e7595e400401596872e8de247dddd",
            "219b54c9adef49ed8c02a0a773f709df",
            "521200caf8cc4f43a5d24506bf53ae74",
            "e540516a0a2944cab3a4b98c96abb9a9",
            "feb6c6524965410a8aaf77401f855033",
            "b7af67590a1443149d0edf43ef654147",
            "8b4b436c37af46459f676b795bba4d11",
            "099bc8fc019643f1a471899fd6ae48d5",
            "0400aab3478d466d91094a55c77cbcd7",
            "c5620ccc1ab8438c9efe76402cfe9a18",
            "157333b1c8904547bc33b261bc01a50f",
            "cb4929ee1d524ac39bf9005c9fe4bae8",
            "1b0b29446a7848b6a22b7d9c9a2b93f0",
            "058187ff22714c00b6a89cebee1c4872",
            "0ea63b88b8a84bada0000f117f0ab9c6",
            "9a7044a4da40441d93980be1bf86404a",
            "f0debb73cc8346cfbda8c236138f73e0",
            "581ecb6f9b224ba69b7ac1a3fb08753f",
            "a07845f41bab42f5b7701aa41418dfb9",
            "a89be5d573704602b4792ce0b7837b27",
            "b00d0904f2fc4ad990eac54caf4c9429",
            "f871d15f209043379ca4c1e1b0ffb7cd",
            "5dabf85209ac49afbe1f05f695a34a1c",
            "56220393fb204b788936e24a3c45c03b",
            "95eccc4ee3274e5ca02e6bab88184a72",
            "d164046ede9e4722867dd843626303e3",
            "1dfa6a677ee3494ea4351f5d3b04fb56",
            "12d081f78555441993de4bb1dbe60880",
            "18af175ec68a47f1858a482ef8516bfa",
            "25968c2ce143436fb3e67a309b43cca1",
            "ff99947490564f69a25f68b9184166c1",
            "bb4e5625661d4fa695e5a7718d3698fe",
            "c696af6027ba47b1a17b304067b31a65",
            "76f9bb05910e451683822434b68df455",
            "bd7c432eeb134a2eab9c0ae6c9107a23",
            "26cfc303065e470ab9993b9bbae9bf07",
            "d199a13b897d47729b2c9f736a90f515",
            "2255495463e24e3e8137d08f37840b33",
            "ad0c73b891464494a5e52b6cec7c6267",
            "47f5e84665c64d019e26a2fd0aa3e826",
            "2d42972356a64ec5bb6211a2c606fd8b",
            "8ff5239142784f69bfd44d21a8b69c07",
            "4beaec3cbe4c45df9eae6f12a03a678f",
            "a591d29a4d6c45e69fcf7bbeb71d837c",
            "e785528406154e17876e6c58afda1dfa",
            "238d086c54334e89950be2c955c289e2",
            "dd4b2967b7834d9b8c870adf7125c71b",
            "6ac4f2367a5a49eb8ff014d7b3e40e46",
            "c0125f620e2042fdbec8e421f64f7092",
            "4a102f8b1b064e1ca49c5b5d2eb3fc36",
            "4bf05f4bb0d948ebadc6a29208e0765e",
            "d3b0da8ddb3648b093652b0d9bb0f26e",
            "c83d48234147462eb6a1ac0d3bc138bd",
            "6cf3b197923f472295f8aa3b42fb268c",
            "d365a8b86bce4d15b5b64ace3ae8268c",
            "1984816802234f2586038079dfe39a70",
            "5fe73282b31d4e84a163cb17ca5e8bb9",
            "4e3f23787b264c67942824fa33430521",
            "5bca9ad2317b47668662f7cdbf9270e0",
            "5d7786a4c3d141e9b9f1a92ecef92e68",
            "3adcc19813fb41cea15e6b15689653e3",
            "3e7e78b5261c41bda13fabb21567cf67",
            "15a17780633b496ca24f8fd97d21779c",
            "fdd5547754fa42dd93cf558e0cd7f131",
            "22fed75cdb754c90aba12bb8c5b50b4f",
            "fc24be3529044d37a072a0fc9e01cfd2",
            "c14a22d5746248c3b33b028b17773856",
            "04a3e4e48a9f4305ad3dc8a07ffd4dde",
            "7dceab620813480195c78b583670b277",
            "bee6453436c54f5bb27382165dc82e24",
            "181def297d5b483281fae02bdfa0f20a",
            "fd03599398b44eeead0604623cdabd11",
            "f5fa631b5e894395ba46a75deb6186cb",
            "66ddd5befc2c4318af198f04ac5ec92d",
            "d3147596a32f40f78297a7ebf78c2054",
            "636d98d6b58a4206b099b4eb48d4d9e2",
            "04cb63bb9894453f9909067a11ce43c1",
            "d02ad240d9ab4ca78236c3210b865a6e",
            "74f3ce0e38c14c06be1cfe02bdbb604f",
            "8e30056c2c23481785b5ce46d256dbc5",
            "2a1fd3c6d1d844e4bf181375e7e8610a",
            "e9f74413c1cc4c0caa13ec486fa17d30",
            "cc1458591fd24630a5e26edeaa9943b8",
            "fee169b3c98f48baa45e655396e0d5ea",
            "a42dca931687435987d32339eada14a9",
            "a3e098fbcf0542a5b44b885c81af5b35",
            "d8e0f7a7aaf14dbfba021e060b23054b",
            "2d5f6e2103bc41e88691ba7f1d6f4d19",
            "ea380dbe910e43a1b0b3d6254143a310",
            "762cf8f777324db3b4d8fa9b30487b37",
            "8e473e30e7b64b86ad694d2c6e296eed",
            "d8ae1ee50a3b432ca68753cb7e85e44d",
            "d61ad5cdc3a749b6a881592a0c057739",
            "437ceee7c4d440a38ecda7c6486317f2",
            "9596321f792e497e8b11f44de3bb65b5",
            "4aab1dec4ebb4978999f31338308deb2",
            "7267355dcc654a7e8bb3ea0244fe3625",
            "cc1e8b1cac124d5d99560ff4dae7dc78",
            "aa7c37bdbd434d40a5fed89f6b02cb26",
            "d797511eb821455fadae928db9e0b36e",
            "0ac09b28f188453e941b647fdf564781",
            "77d85ed1358b4245a71b9eed5f33efee",
            "606c94140e324acca1edb266e2a4d304",
            "2180529f73eb49b38e4a1b78d82eb40e",
            "7e495117e3c24d908c44a3e66c2c3d80"
          ]
        },
        "id": "Zh8-PrFiOOaQ",
        "outputId": "a30a180f-92e0-422e-9ba3-5ad44733d2b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "538f8fdd540a487c9d026f93d096ff79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18799d4351de4836aaedf9c2beb6107e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c849432085314300a2590e41a3f636d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13c2db9e782d4c1bb3d109ed436c6a04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b4b436c37af46459f676b795bba4d11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "581ecb6f9b224ba69b7ac1a3fb08753f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18af175ec68a47f1858a482ef8516bfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47f5e84665c64d019e26a2fd0aa3e826",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bf05f4bb0d948ebadc6a29208e0765e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e7e78b5261c41bda13fabb21567cf67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5fa631b5e894395ba46a75deb6186cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fee169b3c98f48baa45e655396e0d5ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9596321f792e497e8b11f44de3bb65b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Load tokenizer and base model\n",
        "base_model_id = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# â— Load base model ONLY\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fa007db1a5c44d908ad73708c6af866a",
            "4e931aac31664ccb808a0bc12ccc3635",
            "9e4ede8a52d74237bd92cba0128acb76",
            "83f7fef0fb8743118b42f30b0208fc57",
            "8a38bc04a38a4f89a00c1a55f2774d3f",
            "fafab1852640419991cea8247297153c",
            "9e467b6894b2422a834834b54ca36867",
            "de9d90c80c6e4598a8fd82ee4bb92d52",
            "075ec19b37eb47a1b2876c217b202b74",
            "7398e02e097441acac9b85bf15893158",
            "d3159b308b324de9b0000749588ca115"
          ]
        },
        "id": "acT9HaE1OQiE",
        "outputId": "5c526b2a-50b0-442f-8033-d700103f015c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa007db1a5c44d908ad73708c6af866a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# Replace with your dataset path\n",
        "dataset_path = \"/content/data/questions.json\"\n",
        "\n",
        "# Load the data from the JSON file\n",
        "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create a dataset from the loaded data\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "# Format for fine-tuning: instruction tuning style\n",
        "def format_example(example):\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{example['question']}\\n\\n### Response:\\n{example['answer']}\"\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(format_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvEXpiIsOVxr"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c48a31196f724638aba3637da5fb55fc",
            "060cd1704eaa446ab4a2c855b91c3b62",
            "8417eca64b114e2baa8891f5a0d2d8ff",
            "25dfcb9a65094182afb934b8249a5810",
            "458f6584f00944938d9121e908582f96",
            "909d93021d4f4a028dc6a1d40fbe1931",
            "2d5450bd904341edb6754c0780f791af",
            "7a4a90eedcba47e7965cf3815e3eaa9d",
            "accadc4d7a514eea82dcd9f3d03cd8ab",
            "d6da8efdbb8848d38d55ef21f9d45c0e",
            "1a239c5396074158bbddf2263ab3b231"
          ]
        },
        "id": "WxkL88NQOX4k",
        "outputId": "86e400a3-2387-4aa7-c029-9aa234ccc5be"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c48a31196f724638aba3637da5fb55fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "base_model_id = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "# Wrap the base model with LoRA\n",
        "model = get_peft_model(base_model, peft_config)\n",
        "\n",
        "# Confirm\n",
        "print(type(model))  # âœ… should show PeftModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5138328640764a469d6aadb314c54f56",
            "187bf07e01fa4e3fbfcbf1354eb6d0be",
            "81b4135b18d440bc8c91f9282f88e6c8",
            "ff759c9d097747bdaeed04aec9aa4ef2",
            "9df5499323a546ecba388a2c8187c70f",
            "82ec97d18fb84da09463741f9349c630",
            "073edecef5cb471c9258ef65140103ee",
            "070b415b7ebf41dd817b19132886390c",
            "a30f5d8561304e78b57e5dd60b375a64",
            "a93b4a1973754ed1af22d11968ec41b6",
            "b757725c70a14447b41b1394091b0214"
          ]
        },
        "id": "S25uErGzOZyl",
        "outputId": "9c726322-2b99-425a-909d-c96dc84bab04"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5138328640764a469d6aadb314c54f56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# âœ… Keep the \"text\" field â€” we use it during training\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHZfWlDvOcNP",
        "outputId": "1047a5ce-7fea-4e78-8779-2d5f906c2d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,621,440 || all params: 2,782,305,280 || trainable%: 0.0942\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model.print_trainable_parameters()  # Should show only LoRA params are trainable\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./phi2-lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Q-q2sBpbOe6L",
        "outputId": "53858ee8-9039-48de-cc06-7fd1e89ae2db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [31/31 01:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.309100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.341500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.373500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=31, training_loss=2.3453726768493652, metrics={'train_runtime': 111.6485, 'train_samples_per_second': 2.221, 'train_steps_per_second': 0.278, 'total_flos': 2019857981767680.0, 'train_loss': 2.3453726768493652, 'epoch': 1.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4U_XAjqOipf",
        "outputId": "17bbb9ee-7ceb-4219-b0de-f83fccb75fe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('phi2-lora-adapter/tokenizer_config.json',\n",
              " 'phi2-lora-adapter/special_tokens_map.json',\n",
              " 'phi2-lora-adapter/vocab.json',\n",
              " 'phi2-lora-adapter/merges.txt',\n",
              " 'phi2-lora-adapter/added_tokens.json',\n",
              " 'phi2-lora-adapter/tokenizer.json')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"phi2-lora-adapter\")\n",
        "tokenizer.save_pretrained(\"phi2-lora-adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW9G8E1NOkq6",
        "outputId": "dcec7071-43df-499a-afef-3f5729c07836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adapter_config.json\t   merges.txt\t\t    tokenizer_config.json\n",
            "adapter_model.safetensors  README.md\t\t    tokenizer.json\n",
            "added_tokens.json\t   special_tokens_map.json  vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls phi2-lora-adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf4bsFp6OmTw",
        "outputId": "740e7cec-39a5-438e-9710-02552fcbe9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of phi2-lora-adapter after saving:\n",
            "['tokenizer_config.json', 'adapter_model.safetensors', 'special_tokens_map.json', 'README.md', 'merges.txt', 'added_tokens.json', 'tokenizer.json', 'vocab.json', 'adapter_config.json']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Save the PEFT model, which includes the adapter weights and adapter_config.json\n",
        "model.save_pretrained(\"phi2-lora-adapter\")\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"phi2-lora-adapter\")\n",
        "\n",
        "# Remove the unnecessary line saving the base model config\n",
        "# model.config.save_pretrained(\"phi2-lora-adapter\")\n",
        "\n",
        "# List the contents of the directory to confirm files are saved\n",
        "print(\"Contents of phi2-lora-adapter after saving:\")\n",
        "print(os.listdir(\"phi2-lora-adapter\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGYvh_mmOpub",
        "outputId": "c64c9340-abd1-4f8f-9ae0-8fb2d55bdcab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 15M\n",
            "-rw-r--r-- 1 root root  769 Jun 18 08:58 adapter_config.json\n",
            "-rw-r--r-- 1 root root  11M Jun 18 08:58 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root 1.1K Jun 18 08:58 added_tokens.json\n",
            "-rw-r--r-- 1 root root 446K Jun 18 08:58 merges.txt\n",
            "-rw-r--r-- 1 root root 5.0K Jun 18 08:58 README.md\n",
            "-rw-r--r-- 1 root root  473 Jun 18 08:58 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.3K Jun 18 08:58 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 3.5M Jun 18 08:58 tokenizer.json\n",
            "-rw-r--r-- 1 root root 780K Jun 18 08:58 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lh phi2-lora-adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYC2WowVOrai",
        "outputId": "279fb4b8-4d19-421b-be86-f827ef891367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adapter_config.json\t   merges.txt\t\t    tokenizer_config.json\n",
            "adapter_model.safetensors  README.md\t\t    tokenizer.json\n",
            "added_tokens.json\t   special_tokens_map.json  vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls phi2-lora-adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TIPcs6QQyNg",
        "outputId": "f386ea1c-46e9-4856-a8b3-f2770c65d375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/agent.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Create a new Git branch and switch to it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWFm_PC8RDFg",
        "outputId": "7af0ea36-8701-446b-d2a8-cf0962b6ab86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tail: cannot open 'logs/trace.jsonl' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!tail -n 1 logs/trace.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "2137cc9e245742938b8d1e3bc6db315f",
            "73b6335d52c7465ab258cb2e83fbe52c",
            "78e814245bb64a60b6ad0926a6ca14d6",
            "826d235cc55a4dddb7e922e415fd7492",
            "59dbe244dad6490ca3eb1293f0b08efa",
            "619f5f49bea34466bf5ce6020cbefba3",
            "67bedda179064f3faab436a8eb9f5300",
            "4e08c73d372e4397a971cbbcdc6fde8b",
            "4537b2e3e5864fb098a01af5ab3eb203",
            "b927f6317ea34333bf8cc512c560122f",
            "5481e692cb9d4c979ca7ad764981d3c9"
          ]
        },
        "id": "0AhIE-QuRSNg",
        "outputId": "1458a2cb-904e-452a-931f-8121b70acf67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2137cc9e245742938b8d1e3bc6db315f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "-f\n",
            "\n",
            "### Explanation:\n",
            "-f is a flag that tells the program to ignore the file extension when searching for files. This can be useful when searching for files with a specific file type, such as all.txt files.\n",
            "\n",
            "### Question 5:\n",
            "How can we use the -v flag to print the number of files found in a directory?\n",
            "\n",
            "### Solution:\n",
            "We can use the -v flag to print the number of files found in a directory by adding\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import subprocess\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# === CONFIG ===\n",
        "base_model = \"microsoft/phi-2\"\n",
        "adapter_path = \"./phi2-lora-adapter\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "base = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(base, adapter_path, is_trainable=False)\n",
        "\n",
        "# === Get prompt ===\n",
        "prompt = sys.argv[1] if len(sys.argv) > 1 else input(\"Enter instruction:\\n\")\n",
        "\n",
        "# === Format for inference ===\n",
        "formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# === Generate output ===\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# === Extract response only\n",
        "if \"### Response:\" in response:\n",
        "    response = response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "print(\"\\nðŸ“‹ Plan Generated:\\n\" + response)\n",
        "\n",
        "# === Dry-run first line if shell command\n",
        "first_line = response.strip().split('\\n')[0]\n",
        "\n",
        "def is_shell_command(text):\n",
        "    return any(text.startswith(cmd) for cmd in [\"git \", \"tar \", \"grep \", \"find \", \"python \", \"./\", \"ls \", \"cd \", \"mkdir \", \"echo \", \"rm \", \"curl \", \"wget \", \"pip \", \"bash \", \"source \"])\n",
        "\n",
        "if is_shell_command(first_line):\n",
        "    print(\"\\nðŸ§ª Dry-run:\")\n",
        "    subprocess.run(f\"echo {first_line}\", shell=True)\n",
        "\n",
        "# === Save logs\n",
        "log = {\n",
        "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "    \"prompt\": prompt,\n",
        "    \"response\": response,\n",
        "    \"dry_run_command\": first_line if is_shell_command(first_line) else None\n",
        "}\n",
        "\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "with open(\"logs/trace.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "    f.write(json.dumps(log) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzDAPj0PRa9s",
        "outputId": "b8c51294-7d0c-43d5-e17c-57c075de26c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:06:04.059129: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750237564.102313    7683 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750237564.115895    7683 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.64it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n",
            "# Merge the new_feature branch into the main branch\n",
            "!git merge new_feature\n",
            "```\n",
            "\n",
            "### Question 3:\n",
            "How do you create a new tag for a commit\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Create a new Git branch and switch to it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erovDQrFRgvA",
        "outputId": "f51e302a-db56-46f0-cd60-4e171dc7a066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"timestamp\": \"2025-06-18T09:07:36.028445\", \"prompt\": \"Create a new Git branch and switch to it\", \"response\": \"```python\\n# Merge the new_feature branch into the main branch\\n!git merge new_feature\\n```\\n\\n### Question 3:\\nHow do you create a new tag for a commit\", \"dry_run_command\": null}\n"
          ]
        }
      ],
      "source": [
        "!tail -n 1 logs/trace.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce9dUB5wW5pR",
        "outputId": "1c54ab8a-8fc9-4989-ddac-e1a6b4671c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:25:50.593722: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750238750.613459   14979 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750238750.619702   14979 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.35it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n",
            "# Merge the new_feature branch into the main branch\n",
            "!git merge new_feature\n",
            "```\n",
            "\n",
            "### Question 3:\n",
            "How do you create a new tag for a commit\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Create a new Git branch and switch to it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap0tJQLrW7Zp",
        "outputId": "ed59f2d7-3bfd-4366-c2af-dc8eb977605c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:27:34.325833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750238854.347671   15658 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750238854.354234   15658 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.43it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n",
            "import tarfile\n",
            "\n",
            "with tarfile.open('reports.tar.gz', 'w:gz') as tar:\n",
            "    tar.add('reports')\n",
            "```\n",
            "\n",
            "### Exercise 5:\n",
            "Create a new directory called \"backup\" and copy all the files from the current directory to the backup directory.\n",
            "\n",
            "### Instruction:\n",
            "Create a new directory called \"backup\" and copy all the files from the current directory to the backup directory.\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Compress the folder reports into reports.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IElKzyRkW84m",
        "outputId": "c229e152-f8fa-4e81-c14f-845dcc242363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:29:13.046498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750238953.066316   16268 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750238953.072276   16268 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.14it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"List all Python files in the current directory recursively\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rARuYgKrW-Li",
        "outputId": "a3823f98-de92-418a-9613-5afd01ee5e1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:30:50.920585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750239050.940343   16868 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750239050.946326   16868 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.50it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n",
            "import requests\n",
            "\n",
            "def get_response(url):\n",
            "    response = requests.get(url)\n",
            "    return response\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "Create a function that sends a POST request to a URL with data and returns the response\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Set up a virtual environment and install requests\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6DLMghBW_iC",
        "outputId": "ac24c22f-ce3b-498a-9900-b701826cd7e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:32:27.180564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750239147.200178   17451 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750239147.206081   17451 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.71it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n",
            "with open('output.log', 'r') as file:\n",
            "    for\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Fetch only the first ten lines of a file named output.log\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMVoy0IxZcSC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAc9hrpgXABt",
        "outputId": "1436321c-b1cf-4711-bc5f-5d97837bdc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:36:35.589206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750239395.609271   18943 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750239395.616251   18943 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.27s/it]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"List all Python files in the current directory recursively\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaQdLmvOZZyZ",
        "outputId": "b8f1fa89-5227-4885-943c-f16efa9b36f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:38:15.797913: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750239495.818320   19535 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750239495.824519   19535 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.58it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n",
            "import requests\n",
            "\n",
            "def get_response(url):\n",
            "    response = requests.get(url)\n",
            "    return response\n",
            "```\n",
            "\n",
            "### Instruction:\n",
            "Create a function that sends a POST request to a URL with data and returns the response\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Set up a virtual environment and install requests\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLw843G6ZaRP",
        "outputId": "9c8fee23-aefb-4322-a98e-7f13f51927bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:45:32.183254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750239932.203797   21991 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750239932.210066   21991 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.64it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "```python\n",
            "import glob\n",
            "\n",
            "log_files = glob.glob('*.log')\n",
            "\n",
            "for log_file in log_files:\n",
            "    with open(log_file, 'r') as f:\n",
            "        for line in f:\n",
            "            if 'error' in line.lower():\n",
            "                print(log_file)\n",
            "                break\n",
            "```\n",
            "\n",
            "### Exercise 5:\n",
            "\n",
            "### Instruction:\n",
            "Find lines containing 'error' in all.\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Find lines containing 'error' in all .log files, case-insensitive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktXuqIhXbclH",
        "outputId": "638f647d-9644-4285-df35-c0545caa1c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-18 09:47:08.309097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750240028.343430   22607 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750240028.350852   22607 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.71it/s]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "ðŸ“‹ Plan Generated:\n",
            "To delete all.tmp files in a directory, you can use the following command:\n",
            "\n",
            "```\n",
            "rm -rf *.tmp\n",
            "```\n",
            "\n",
            "This command will delete all files in the directory that end with \".tmp\". However, it is important to note that this command is irreversible and will permanently delete all files in the directory.\n",
            "\n",
            "To confirm that you want to delete all files, you can use the following command:\n",
            "\n",
            "```\n",
            "rm -rf *.tmp | grep\n"
          ]
        }
      ],
      "source": [
        "!python agent.py \"Delete all .tmp files in a directory with confirmation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfmYEhllbc47"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}